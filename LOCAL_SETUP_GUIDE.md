# ðŸ  GUIA DE SETUP LOCAL - HARDWARE DEDICADO

## ðŸŽ¯ VISÃƒO GERAL

Este projeto foi configurado para rodar **100% LOCAL** usando seu hardware:
- **2x Intel i9** (CPUs)
- **3x NVIDIA RTX 3090** (GPUs com 24GB VRAM cada)
- **0 APIs pagas** (sem OpenAI, Anthropic, Pinecone)

---

## ðŸ“¦ STACK LOCAL

| Componente | Tecnologia | LocalizaÃ§Ã£o |
|------------|-----------|-------------|
| **LLM Principal** | Llama 3.1 70B | GPU 0 (via Ollama) |
| **LLM RÃ¡pido** | Llama 3.1 8B | GPU 1 (via Ollama) |
| **Vision AI** | LLaVA 13B | GPU 2 (via Ollama) |
| **Embeddings** | nomic-embed-text | CPU/GPU compartilhado |
| **Vector DB** | ChromaDB | Docker (localhost:8000) |
| **Cache** | Redis | Docker (localhost:6379) |
| **Database** | PostgreSQL | Docker (localhost:5432) |

---

## ðŸš€ INSTALAÃ‡ÃƒO PASSO A PASSO

### **PASSO 1: Instalar Ollama (Gerenciador de LLMs)**

```bash
# Windows (PowerShell como Admin)
winget install Ollama.Ollama

# OU baixar instalador:
# https://ollama.com/download/windows

# Linux
curl -fsSL https://ollama.com/install.sh | sh

# Verificar instalaÃ§Ã£o
ollama --version
```

### **PASSO 2: Pull dos Modelos de IA**

```bash
# LLM Principal (Llama 3.1 70B) - ~40GB
ollama pull llama3.1:70b

# LLM RÃ¡pido (Llama 3.1 8B) - ~4.7GB
ollama pull llama3.1:8b

# Vision (LLaVA 13B) - ~8GB
ollama pull llava:13b

# Embeddings (Nomic Embed) - ~274MB
ollama pull nomic-embed-text
```

**âš ï¸ IMPORTANTE:**
- O download total Ã© ~53GB
- O Llama 70B precisa de ~48GB de VRAM livre (2x RTX 3090)
- Deixe os downloads rodando (pode levar 30-60 min)

### **PASSO 3: Configurar VariÃ¡veis de Ambiente**

```bash
# Copiar template
cp .env.local.example .env.local

# Editar .env.local com suas configuraÃ§Ãµes
```

**VariÃ¡veis principais:**
```bash
# Database
DATABASE_URL="postgresql://nfc:sua_senha@localhost:5432/nfc_admin"

# Ollama
OLLAMA_URL="http://localhost:11434"
DEFAULT_LLM_MODEL="llama3.1:70b"
FAST_LLM_MODEL="llama3.1:8b"
VISION_MODEL="llava:13b"

# ChromaDB
CHROMADB_URL="http://localhost:8000"

# GPUs
CUDA_VISIBLE_DEVICES="0,1,2"
GPU_LLM_MAIN="0"
GPU_LLM_FAST="1"
GPU_VISION="2"
```

### **PASSO 4: Iniciar ServiÃ§os (Docker)**

```bash
# Subir PostgreSQL + Redis + ChromaDB
docker-compose up -d

# Verificar que todos estÃ£o rodando
docker-compose ps
```

**ServiÃ§os esperados:**
- âœ… `nfc-postgres` (porta 5432)
- âœ… `nfc-redis` (porta 6379)
- âœ… `nfc-chromadb` (porta 8000)

### **PASSO 5: Setup do Banco de Dados**

```bash
# Instalar dependÃªncias
npm install

# Gerar Prisma Client
npx prisma generate

# Criar database e tabelas
npx prisma migrate dev --name init

# Popular com dados iniciais
npx ts-node prisma/seed.ts
```

### **PASSO 6: Iniciar AplicaÃ§Ã£o**

```bash
# Desenvolvimento
npm run dev

# AplicaÃ§Ã£o estarÃ¡ em: http://localhost:3001
```

**Login padrÃ£o:**
- Email: `admin@nutrifitcoach.com`
- Senha: `admin123`

---

## ðŸŽ® GERENCIAMENTO DE GPUs

### **DistribuiÃ§Ã£o Recomendada**

```
GPU 0 (24GB) â†’ Llama 3.1 70B (Modelo Principal)
GPU 1 (24GB) â†’ Llama 3.1 8B (Respostas RÃ¡pidas)
GPU 2 (24GB) â†’ LLaVA 13B (AnÃ¡lise de Imagens)
```

### **Configurar Ollama para usar GPUs especÃ­ficas**

```bash
# Windows (PowerShell)
$env:CUDA_VISIBLE_DEVICES="0,1"
ollama serve

# Linux/Mac
CUDA_VISIBLE_DEVICES=0,1 ollama serve
```

### **Monitorar GPUs em Tempo Real**

```bash
# Comando nvidia-smi watch
watch -n 1 nvidia-smi

# OU use o Dashboard (widget GPU Monitor)
```

---

## ðŸ§ª TESTAR COMPONENTES

### **1. Testar Ollama**

```bash
# Testar Llama 70B
curl http://localhost:11434/api/generate -d '{
  "model": "llama3.1:70b",
  "prompt": "Explique proteÃ­na em 3 linhas",
  "stream": false
}'

# Testar Vision (LLaVA)
ollama run llava:13b
# >>> /help para comandos
```

### **2. Testar ChromaDB**

```bash
curl http://localhost:8000/api/v1/heartbeat

# Deve retornar: {"nanosecond heartbeat": ...}
```

### **3. Testar Embeddings**

```bash
curl http://localhost:11434/api/embeddings -d '{
  "model": "nomic-embed-text",
  "prompt": "teste de embedding"
}'
```

---

## ðŸ“Š PERFORMANCE ESPERADA

| Modelo | VRAM Usada | Tokens/seg | LatÃªncia (primeira resposta) |
|--------|-----------|------------|------------------------------|
| **Llama 70B** | ~45GB | 15-20 | ~3-5s |
| **Llama 8B** | ~5GB | 80-100 | <1s |
| **LLaVA 13B** | ~12GB | 10-15 | ~5-8s |

**Dicas de OtimizaÃ§Ã£o:**
- Use `llama3.1:8b` para respostas rÃ¡pidas (flag `useFastModel: true`)
- Configure `aiCooldown` nas Arenas para evitar spam de GPU
- Cache de embeddings no Redis reduz carga

---

## ðŸ› TROUBLESHOOTING

### **Problema: "Ollama not available"**

```bash
# Verificar se Ollama estÃ¡ rodando
ollama list

# Reiniciar Ollama
# Windows: Fechar pela bandeja e abrir novamente
# Linux:
sudo systemctl restart ollama
```

### **Problema: "CUDA out of memory"**

**Causa:** Modelo muito grande para a VRAM disponÃ­vel.

**SoluÃ§Ãµes:**
1. Usar modelo menor: `llama3.1:8b` em vez de `70b`
2. Fechar outros processos usando GPU
3. Configurar quantizaÃ§Ã£o:
   ```bash
   # Pull versÃ£o quantizada (menor precisÃ£o, menos VRAM)
   ollama pull llama3.1:70b-q4_0
   ```

### **Problema: Respostas lentas**

1. **Verificar uso de GPU:**
   ```bash
   nvidia-smi
   # GPU Util deve estar 90-100% durante geraÃ§Ã£o
   ```

2. **Usar modelo mais rÃ¡pido:**
   ```typescript
   // No cÃ³digo, flag para usar Llama 8B
   useFastModel: true
   ```

3. **Reduzir `num_predict` (max tokens):**
   ```typescript
   // lib/ai/llm.ts
   options: {
     num_predict: 300 // era 500
   }
   ```

### **Problema: ChromaDB nÃ£o conecta**

```bash
# Verificar se container estÃ¡ rodando
docker ps | grep chromadb

# Ver logs
docker logs nfc-chromadb

# Reiniciar
docker-compose restart chromadb
```

---

## ðŸ”§ COMANDOS ÃšTEIS

### **Ollama**
```bash
ollama list                    # Listar modelos instalados
ollama rm llama3.1:70b        # Remover modelo
ollama ps                      # Ver modelos em execuÃ§Ã£o
ollama serve                   # Iniciar servidor manualmente
```

### **Docker**
```bash
docker-compose up -d           # Subir todos os serviÃ§os
docker-compose down            # Parar todos
docker-compose logs -f         # Ver logs em tempo real
docker-compose restart redis   # Reiniciar serviÃ§o especÃ­fico
```

### **GPU Monitoring**
```bash
nvidia-smi                     # Status instantÃ¢neo
watch -n 1 nvidia-smi         # Update a cada 1s
nvidia-smi dmon               # Device monitoring contÃ­nuo
```

---

## ðŸ“ˆ PRÃ“XIMAS OTIMIZAÃ‡Ã•ES

- [ ] Implementar **vLLM** para inferÃªncia ainda mais rÃ¡pida
- [ ] Usar **TensorRT** para otimizar modelos
- [ ] Configurar **KV cache** para respostas sequenciais
- [ ] Load balancing automÃ¡tico entre GPUs
- [ ] QuantizaÃ§Ã£o dinÃ¢mica baseada em carga

---

## ðŸ†˜ SUPORTE

**DocumentaÃ§Ã£o:**
- Ollama: https://ollama.com/docs
- ChromaDB: https://docs.trychroma.com
- LLaVA: https://llava-vl.github.io

**Troubleshooting:**
- Ver logs: `docker-compose logs`
- Dashboard: GPU Monitor widget
- Healthcheck: `/api/hardware/gpu`

---

## âœ… CHECKLIST FINAL

Antes de comeÃ§ar a usar:

- [ ] Ollama instalado e rodando
- [ ] Modelos baixados (llama3.1:70b, 8b, llava, nomic-embed)
- [ ] Docker Compose rodando (postgres, redis, chromadb)
- [ ] VariÃ¡veis de ambiente configuradas
- [ ] Banco de dados migrado e populado
- [ ] AplicaÃ§Ã£o rodando em localhost:3001
- [ ] GPU Monitor mostrando 3 GPUs

**Status esperado no Dashboard:**
- âœ… Database: Operacional
- âœ… Redis Cache: Operacional
- âœ… Ollama (Llama 3.1 70B): Local - GPU 0
- âœ… ChromaDB (RAG): Local - Docker
- âœ… LLaVA Vision: Local - GPU 2

---

ðŸŽ‰ **Setup Completo! VocÃª agora tem um sistema de IA 100% local e poderoso!**
